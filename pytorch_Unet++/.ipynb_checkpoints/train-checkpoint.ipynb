{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "from optparse import OptionParser\n",
    "import numpy as np\n",
    "import pdb\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import shutil\n",
    "from eval import eval_net\n",
    "from unet import UNet\n",
    "from utils import get_ids, split_ids, split_train_val, get_imgs_and_masks, batch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'runs/'\n",
    "ct = time.localtime(time.time())\n",
    "directory = os.path.join(directory, \"%04d-%02d-%02d, %02d:%02d:%02d_bce+dice/\" %\n",
    "                                                (ct.tm_year, ct.tm_mon, ct.tm_mday, ct.tm_hour, ct.tm_min, ct.tm_sec))\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "writer = SummaryWriter(directory)\n",
    "\n",
    "dir_model = 'fullmodel/'\n",
    "if not os.path.exists(dir_model):\n",
    "    os.makedirs(dir_model)\n",
    "\n",
    "best_dice = 0\n",
    "best_loss = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(net,\n",
    "              epochs=5,\n",
    "              batch_size=1,\n",
    "              lr=0.1,\n",
    "              val_percent=0.05,\n",
    "              save_cp=True,\n",
    "              gpu=False,\n",
    "              img_scale=0.5):\n",
    "\n",
    "    global best_dice, best_loss\n",
    "    dir_img = '/home/mori/Programming/Net_Pruning/unetdataset_patchImg/img/'\n",
    "    dir_mask = '/home/mori/Programming/Net_Pruning/unetdataset_patchImg/graylabel/'\n",
    "\n",
    "\n",
    "    ids = get_ids(dir_img)  # get file name (without .png)\n",
    "    print(\"ids:{}\".format(ids))\n",
    "    \n",
    "    ids = split_ids(ids)  # 重采样？\n",
    "    print(\"ids:{}\".format(ids))\n",
    "    iddataset = split_train_val(ids, val_percent) # 按给定比例划分打乱的数据集\n",
    "\n",
    "    print('''\n",
    "    Starting training:\n",
    "        Epochs: {}\n",
    "        Batch size: {}\n",
    "        Learning rate: {}\n",
    "        Training size: {}\n",
    "        Validation size: {}\n",
    "        Checkpoints: {}\n",
    "        CUDA: {}\n",
    "    '''.format(epochs, batch_size, lr, len(iddataset['train']),\n",
    "               len(iddataset['val']), str(save_cp), str(gpu)))\n",
    "\n",
    "    N_train = len(iddataset['train'])\n",
    "    print(\"N_train:{}\".format(N_train))\n",
    "    optimizer = optim.SGD(net.parameters(),\n",
    "                          lr=lr,\n",
    "                          momentum=0.9,\n",
    "                          weight_decay=0.0005)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print('Starting epoch {}/{}.'.format(epoch + 1, epochs))\n",
    "        net.train()\n",
    "\n",
    "        New_lr=adjust_learning_rate(optimizer, epoch,epochs)\n",
    "        print(' lr: {}'.format(New_lr))\n",
    "        # reset the generators\n",
    "        train = get_imgs_and_masks(iddataset['train'], dir_img, dir_mask, img_scale)\n",
    "        val = get_imgs_and_masks(iddataset['val'], dir_img, dir_mask, img_scale)\n",
    "\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for i, b in enumerate(batch(train, batch_size)):  # 手动分出batch\n",
    "            imgs = np.array([i[0] for i in b]).astype(np.float32)\n",
    "            true_masks = np.array([i[1] for i in b])\n",
    "\n",
    "            imgs = torch.from_numpy(imgs)\n",
    "            true_masks = torch.from_numpy(true_masks)\n",
    "\n",
    "            if gpu:\n",
    "                imgs = imgs.cuda()\n",
    "                true_masks = true_masks.cuda()\n",
    "\n",
    "            masks_pred = net(imgs)\n",
    "            masks_probs_flat = masks_pred.view(-1)\n",
    "\n",
    "            true_masks_flat = true_masks.view(-1)\n",
    "            true_masks_flat = true_masks_flat/255  # 归一化\n",
    "\n",
    "            loss = criterion(masks_probs_flat, true_masks_flat)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            print('{0:.4f} --- loss: {1:.6f}'.format(i * batch_size / N_train, loss.item()))\n",
    "            newloss=loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print('Epoch finished ! Loss: {}'.format(epoch_loss / i))\n",
    "    \n",
    "        val_dice = eval_net(net, val, gpu)\n",
    "        print('Validation Dice Coeff: {}'.format(val_dice))\n",
    "\n",
    "        writer.add_scalar('train_loss',epoch_loss/i,(epoch+1))\n",
    "        writer.add_scalar('val_dice', val_dice, (epoch+1))\n",
    "\n",
    "\n",
    "        if save_cp:\n",
    "            #torch.save(net.state_dict(),dir_checkpoint + 'CP{}.pth'.format(epoch + 1))\n",
    "            #print('Checkpoint {} saved !'.format(epoch + 1))\n",
    "            dice_best = val_dice > best_dice\n",
    "            loss_best = epoch_loss / i < best_loss\n",
    "            best_dice = max(val_dice, best_dice)\n",
    "            save_checkpoint({\n",
    "                'epoch': epoch + 1,\n",
    "                'state_dict': net.state_dict(),\n",
    "                'best_dice': best_dice,\n",
    "                'best_loss': best_loss, \n",
    "            }, dice_best, loss_best)\n",
    "        \n",
    "    print('Best dice: ', best_dice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch,epochs):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 after 150 and 225 epochs\"\"\"\n",
    "    # lr = args.lr * (0.1 ** (epoch // (20+epochs*0.1+0.12*epoch))*(4**((epoch // (20+epochs*0.1+0.12*epoch))//3)))\n",
    "    if epoch<=int(epochs*0.2):\n",
    "        lr = args.lr\n",
    "    elif epoch>int(epochs*0.2) and epoch<=int(epochs*0.35):\n",
    "        lr = args.lr*0.1*3\n",
    "    elif epoch>int(epochs*0.35) and epoch<=int(epochs*0.5):\n",
    "        lr = args.lr*0.1\n",
    "    elif epoch>int(epochs*0.5) and epoch<=int(epochs*0.65):\n",
    "        lr = args.lr*0.01*3\n",
    "    elif epoch>int(epochs*0.65) and epoch<=int(epochs*0.8):\n",
    "        lr = args.lr*0.01\n",
    "    elif epoch>int(epochs*0.8) and epoch<=int(epochs*0.95):\n",
    "        lr = args.lr*0.001*3\n",
    "    elif epoch>int(epochs*0.95):\n",
    "        lr = args.lr*0.001\n",
    "\n",
    "    writer.add_scalar('learning_rate', lr, epoch)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return lr\n",
    "\n",
    "def save_checkpoint(state, dice_best, loss_best, filename='checkpoint.pth'):\n",
    "    \"\"\"Saves checkpoint to disk\"\"\"\n",
    "    filename = directory + filename\n",
    "    torch.save(state, filename)\n",
    "    if dice_best:\n",
    "        shutil.copyfile(filename,os.path.join(directory, 'dice_best.pth'))\n",
    "    if loss_best:\n",
    "        shutil.copyfile(filename,os.path.join(directory, 'loss_best.pth'))\n",
    "\n",
    "\n",
    "def get_args():\n",
    "    parser = OptionParser()\n",
    "    parser.add_option('-e', '--epochs', dest='epochs', default=1, type='int',\n",
    "                      help='number of epochs')\n",
    "    parser.add_option('-b', '--batch-size', dest='batchsize', default=6,\n",
    "                      type='int', help='batch size')\n",
    "    parser.add_option('-l', '--learning-rate', dest='lr', default=0.1,\n",
    "                      type='float', help='learning rate')\n",
    "    parser.add_option('-g', '--gpu', action='store_true', dest='gpu',\n",
    "                      default=True, help='use cuda')\n",
    "    parser.add_option('-c', '--load', dest='load',\n",
    "                      default=False, help='load file model')\n",
    "    parser.add_option('-s', '--scale', dest='scale', type='float',\n",
    "                      default=0.5, help='downscaling factor of the images')\n",
    "    parser.add_option('--tensorboard', default=True,\n",
    "                    help='Log progress to TensorBoard', action='store_true')\n",
    "\n",
    "    (options, args) = parser.parse_args()\n",
    "    return options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    args = get_args()\n",
    "\n",
    "    net = UNet(n_channels=3, n_classes=1)\n",
    "\n",
    "    if args.load:\n",
    "        if os.path.isfile(args.load):\n",
    "            print(\"=> loading checkpoint '{}'\".format(args.load))\n",
    "            checkpoint = torch.load(args.load)\n",
    "            args.start_epoch = checkpoint['epoch']\n",
    "            best_dice = checkpoint['best_dice']\n",
    "            best_loss = checkpoint['best_loss']\n",
    "            net.load_state_dict(checkpoint['state_dict'])\n",
    "            print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "                    .format(args.load, checkpoint['epoch']))\n",
    "        else:\n",
    "            print(\"=> no checkpoint found at '{}'\".format(args.load))\n",
    "\n",
    "    if args.gpu:\n",
    "        net.cuda()\n",
    "        # cudnn.benchmark = True # faster convolutions, but more memory\n",
    "\n",
    "    dummy_input = torch.rand(1,3,256,256).cuda()\n",
    "    writer.add_graph(net,(dummy_input,))\n",
    "\n",
    "    try:\n",
    "        train_net(net=net,\n",
    "                  epochs=args.epochs,\n",
    "                  batch_size=args.batchsize,\n",
    "                  lr=args.lr,\n",
    "                  gpu=args.gpu,\n",
    "                  img_scale=args.scale)\n",
    "\n",
    "        torch.save(net,\n",
    "                    dir_model + 'CP{}.pth'.format(0))\n",
    "    except KeyboardInterrupt:\n",
    "        torch.save(net.state_dict(), 'INTERRUPTED.pth')\n",
    "        print('Saved interrupt')\n",
    "        try:\n",
    "            sys.exit(0)\n",
    "        except SystemExit:\n",
    "            os._exit(0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Usage: ipykernel_launcher.py [options]\n",
      "\n",
      "ipykernel_launcher.py: error: no such option: -f\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mori/anaconda3/envs/prune/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2969: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# %load train_unet++.py\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "from optparse import OptionParser\n",
    "import numpy as np\n",
    "import pdb\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import shutil\n",
    "from eval import eval_net\n",
    "from unet import UNet\n",
    "from utils import get_ids, split_ids, split_train_val, get_imgs_and_masks, batch\n",
    "from tqdm import tqdm\n",
    "\n",
    "####### set directory ###########\n",
    "##### 1. save for tensorboard ###\n",
    "directory = 'runs/'\n",
    "ct = time.localtime(time.time())\n",
    "directory = os.path.join(directory, \"%04d-%02d-%02d, %02d:%02d:%02d_bce+dice/\" %\n",
    "                                                (ct.tm_year, ct.tm_mon, ct.tm_mday, ct.tm_hour, ct.tm_min, ct.tm_sec))\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "writer = SummaryWriter(directory)\n",
    "\n",
    "\n",
    "#### 2. save model #####\n",
    "dir_model = 'fullmodel/'\n",
    "if not os.path.exists(dir_model):\n",
    "    os.makedirs(dir_model)\n",
    "\n",
    "best_dice = 0\n",
    "best_loss = 10\n",
    "\n",
    "\n",
    "def train_net(net,\n",
    "              epochs=5,\n",
    "              batch_size=1,\n",
    "              lr=0.1,\n",
    "              val_percent=0.05,\n",
    "              save_cp=True,\n",
    "              gpu=False,\n",
    "              img_scale=0.5):\n",
    "\n",
    "    global best_dice, best_loss\n",
    "    dir_img = '/home/mori/Programming/Net_Pruning/unetdataset_patchImg/img/'\n",
    "    dir_mask = '/home/mori/Programming/Net_Pruning/unetdataset_patchImg/graylabel/'\n",
    "\n",
    "\n",
    "    ids = get_ids(dir_img)  # get file name (without .png)\n",
    "    print(\"ids:{}\".format(ids))\n",
    "    \n",
    "    ids = split_ids(ids)  # 重采样？\n",
    "    print(\"ids:{}\".format(ids))\n",
    "    iddataset = split_train_val(ids, val_percent) # 按给定比例划分打乱的数据集\n",
    "\n",
    "    print('''\n",
    "    Starting training:\n",
    "        Epochs: {}\n",
    "        Batch size: {}\n",
    "        Learning rate: {}\n",
    "        Training size: {}\n",
    "        Validation size: {}\n",
    "        Checkpoints: {}\n",
    "        CUDA: {}\n",
    "    '''.format(epochs, batch_size, lr, len(iddataset['train']),\n",
    "               len(iddataset['val']), str(save_cp), str(gpu)))\n",
    "\n",
    "    N_train = len(iddataset['train'])\n",
    "    print(\"N_train:{}\".format(N_train))\n",
    "    optimizer = optim.SGD(net.parameters(),\n",
    "                          lr=lr,\n",
    "                          momentum=0.9,\n",
    "                          weight_decay=0.0005)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print('Starting epoch {}/{}.'.format(epoch + 1, epochs))\n",
    "        net.train()\n",
    "\n",
    "        New_lr=adjust_learning_rate(optimizer, epoch,epochs)\n",
    "        print(' lr: {}'.format(New_lr))\n",
    "        # reset the generators\n",
    "        train = get_imgs_and_masks(iddataset['train'], dir_img, dir_mask, img_scale)\n",
    "        val = get_imgs_and_masks(iddataset['val'], dir_img, dir_mask, img_scale)\n",
    "\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for i, b in enumerate(batch(train, batch_size)):  # 手动分出batch\n",
    "            imgs = np.array([i[0] for i in b]).astype(np.float32)\n",
    "            true_masks = np.array([i[1] for i in b])\n",
    "\n",
    "            imgs = torch.from_numpy(imgs)\n",
    "            true_masks = torch.from_numpy(true_masks)\n",
    "\n",
    "            if gpu:\n",
    "                imgs = imgs.cuda()\n",
    "                true_masks = true_masks.cuda()\n",
    "\n",
    "            masks_pred = net(imgs)\n",
    "            masks_probs_flat = masks_pred.view(-1)\n",
    "\n",
    "            true_masks_flat = true_masks.view(-1)\n",
    "            true_masks_flat = true_masks_flat/255  # 归一化\n",
    "\n",
    "            loss = criterion(masks_probs_flat, true_masks_flat)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            print('{0:.4f} --- loss: {1:.6f}'.format(i * batch_size / N_train, loss.item()))\n",
    "            newloss=loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print('Epoch finished ! Loss: {}'.format(epoch_loss / i))\n",
    "    \n",
    "        val_dice = eval_net(net, val, gpu)\n",
    "        print('Validation Dice Coeff: {}'.format(val_dice))\n",
    "\n",
    "        writer.add_scalar('train_loss',epoch_loss/i,(epoch+1))\n",
    "        writer.add_scalar('val_dice', val_dice, (epoch+1))\n",
    "\n",
    "\n",
    "        if save_cp:\n",
    "            #torch.save(net.state_dict(),dir_checkpoint + 'CP{}.pth'.format(epoch + 1))\n",
    "            #print('Checkpoint {} saved !'.format(epoch + 1))\n",
    "            dice_best = val_dice > best_dice\n",
    "            loss_best = epoch_loss / i < best_loss\n",
    "            best_dice = max(val_dice, best_dice)\n",
    "            save_checkpoint({\n",
    "                'epoch': epoch + 1,\n",
    "                'state_dict': net.state_dict(),\n",
    "                'best_dice': best_dice,\n",
    "                'best_loss': best_loss, \n",
    "            }, dice_best, loss_best)\n",
    "        \n",
    "    print('Best dice: ', best_dice)\n",
    "def adjust_learning_rate(optimizer, epoch,epochs):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 after 150 and 225 epochs\"\"\"\n",
    "    # lr = args.lr * (0.1 ** (epoch // (20+epochs*0.1+0.12*epoch))*(4**((epoch // (20+epochs*0.1+0.12*epoch))//3)))\n",
    "    if epoch<=int(epochs*0.2):\n",
    "        lr = args.lr\n",
    "    elif epoch>int(epochs*0.2) and epoch<=int(epochs*0.35):\n",
    "        lr = args.lr*0.1*3\n",
    "    elif epoch>int(epochs*0.35) and epoch<=int(epochs*0.5):\n",
    "        lr = args.lr*0.1\n",
    "    elif epoch>int(epochs*0.5) and epoch<=int(epochs*0.65):\n",
    "        lr = args.lr*0.01*3\n",
    "    elif epoch>int(epochs*0.65) and epoch<=int(epochs*0.8):\n",
    "        lr = args.lr*0.01\n",
    "    elif epoch>int(epochs*0.8) and epoch<=int(epochs*0.95):\n",
    "        lr = args.lr*0.001*3\n",
    "    elif epoch>int(epochs*0.95):\n",
    "        lr = args.lr*0.001\n",
    "\n",
    "    writer.add_scalar('learning_rate', lr, epoch)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return lr\n",
    "\n",
    "\n",
    "def save_checkpoint(state, dice_best, loss_best, filename='checkpoint.pth'):\n",
    "    \"\"\"Saves checkpoint to disk\"\"\"\n",
    "    filename = directory + filename\n",
    "    torch.save(state, filename)\n",
    "    if dice_best:\n",
    "        shutil.copyfile(filename,os.path.join(directory, 'dice_best.pth'))\n",
    "    if loss_best:\n",
    "        shutil.copyfile(filename,os.path.join(directory, 'loss_best.pth'))\n",
    "\n",
    "\n",
    "def get_args():\n",
    "    parser = OptionParser()\n",
    "    parser.add_option('-e', '--epochs', dest='epochs', default=1, type='int',\n",
    "                      help='number of epochs')\n",
    "    parser.add_option('-b', '--batch-size', dest='batchsize', default=6,\n",
    "                      type='int', help='batch size')\n",
    "    parser.add_option('-l', '--learning-rate', dest='lr', default=0.1,\n",
    "                      type='float', help='learning rate')\n",
    "    parser.add_option('-g', '--gpu', action='store_true', dest='gpu',\n",
    "                      default=True, help='use cuda')\n",
    "    parser.add_option('-c', '--load', dest='load',\n",
    "                      default=False, help='load file model')\n",
    "    parser.add_option('-s', '--scale', dest='scale', type='float',\n",
    "                      default=0.5, help='downscaling factor of the images')\n",
    "    parser.add_option('--tensorboard', default=True,\n",
    "                    help='Log progress to TensorBoard', action='store_true')\n",
    "\n",
    "    (options, args) = parser.parse_args()\n",
    "    return options\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    args = get_args()\n",
    "\n",
    "    net = UNet(n_channels=3, n_classes=1)\n",
    "\n",
    "    if args.load:\n",
    "        if os.path.isfile(args.load):\n",
    "            print(\"=> loading checkpoint '{}'\".format(args.load))\n",
    "            checkpoint = torch.load(args.load)\n",
    "            args.start_epoch = checkpoint['epoch']\n",
    "            best_dice = checkpoint['best_dice']\n",
    "            best_loss = checkpoint['best_loss']\n",
    "            net.load_state_dict(checkpoint['state_dict'])\n",
    "            print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "                    .format(args.load, checkpoint['epoch']))\n",
    "        else:\n",
    "            print(\"=> no checkpoint found at '{}'\".format(args.load))\n",
    "\n",
    "    if args.gpu:\n",
    "        net.cuda()\n",
    "        # cudnn.benchmark = True # faster convolutions, but more memory\n",
    "\n",
    "    dummy_input = torch.rand(1,3,256,256).cuda()\n",
    "    writer.add_graph(net,(dummy_input,))\n",
    "\n",
    "    try:\n",
    "        train_net(net=net,\n",
    "                  epochs=args.epochs,\n",
    "                  batch_size=args.batchsize,\n",
    "                  lr=args.lr,\n",
    "                  gpu=args.gpu,\n",
    "                  img_scale=args.scale)\n",
    "\n",
    "        torch.save(net,\n",
    "                    dir_model + 'CP{}.pth'.format(0))\n",
    "    except KeyboardInterrupt:\n",
    "        torch.save(net.state_dict(), 'INTERRUPTED.pth')\n",
    "        print('Saved interrupt')\n",
    "        try:\n",
    "            sys.exit(0)\n",
    "        except SystemExit:\n",
    "            os._exit(0)\n",
    "    \n",
    "    # writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
